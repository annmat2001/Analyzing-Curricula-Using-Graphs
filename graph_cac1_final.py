# -*- coding: utf-8 -*-
"""Graph_CAC1_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wZ0-UuQnMxBcKnc_hUnoTBqBLHGFpC3r
"""

pip install python-docx

import re
import docx
import json
import os

def extract_course_data(doc_path):
    doc = docx.Document(doc_path)
    courses = {}
    current_course_code = None
    current_unit = None

    # Update the regex to handle cases where course codes and names are separated by a colon, dash, or space.
    course_code_regex = r'^(MDS|MST|MDA)([0-9A-Za-z]+)\s*[:;\-]?\s*(.+)$'
    unit_regex = r'^(?:[Uu][Nn][Ii][Tt])\s+(\d+)\s*[:;\-]?\s*(.+)$'  # Case-insensitive 'UNIT'

    for para in doc.paragraphs:
        text = para.text.strip()

        # Extract course code and name
        course_code_match = re.match(course_code_regex, text)
        if course_code_match:
            current_course_code = course_code_match.group(1) + course_code_match.group(2)
            course_name = course_code_match.group(3).strip()
            courses[current_course_code] = {
                'course_name': course_name,
                'units': {}
            }
            current_unit = None
            continue

        # Extract unit number and name
        unit_match = re.match(unit_regex, text)
        if unit_match and current_course_code:
            unit_number = f"UNIT {unit_match.group(1)}"
            unit_name = unit_match.group(2).strip()
            courses[current_course_code]['units'][unit_number] = {
                'unit_name': unit_name,
                'subtopics': []
            }
            current_unit = unit_number
            continue

        # Extract subtopics
        if current_unit and current_course_code:
            subtopics = re.split(r'(?<!e\.g)\.|[,:;–—\-]', text)
            subtopics = [sub.strip() for sub in subtopics if sub.strip()]
            courses[current_course_code]['units'][current_unit]['subtopics'].extend(subtopics)

    return courses

def get_all_subtopics(courses):
    subtopics = [sub for course in courses.values() for unit in course['units'].values() for sub in unit['subtopics']]
    return sorted(set(subtopics))

def create_lists_and_structure(courses):
    course_codes = list(courses.keys())
    course_names = [data['course_name'] for data in courses.values()]
    unit_names = [unit_data['unit_name'] for course_data in courses.values() for unit_data in course_data['units'].values()]
    subtopics = get_all_subtopics(courses)

    course_structure = {}
    for course_code, course_data in courses.items():
        course_structure[course_code] = {
            'course_name': course_data['course_name'],
            'units': {}
        }
        for unit_number, unit_data in course_data['units'].items():
            course_structure[course_code]['units'][unit_number] = {
                'unit_name': unit_data['unit_name'],
                'subtopics': unit_data['subtopics']
            }

    return course_codes, course_names, unit_names, subtopics, course_structure

# def save_data_to_json(data, filename):
#     with open(filename, 'w') as json_file:
#         json.dump(data, json_file, indent=4)

# Define paths to syllabuses and output directory
doc_paths = {
    "MDS": '/content/MDS.docx',
    "MST": '/content/MST.docx',
    "MDA": '/content/MDA.docx'
}
# output_dir = '/content/ExtractedData/'

# # Ensure the output directory exists
# os.makedirs(output_dir, exist_ok=True)

# Process each syllabus and save the data
for course_type, doc_path in doc_paths.items():
    courses = extract_course_data(doc_path)
    course_codes, course_names, unit_names, subtopics, course_structure = create_lists_and_structure(courses)

    # Print for verification
    print(f"{course_type} Syllabus:")
    print("Course Codes:")
    print(len(course_codes),course_codes)
    print("\nCourse Names:")
    print(len(course_names),course_names)
    print("\nUnit Names:")
    print(len(unit_names),unit_names)
    print("\nSubtopics:")
    print(len(subtopics),subtopics)
    print("\nCourse Structure:")
    print(course_structure)
    print("\n" + "="*50 + "\n")

    # # Save the structured data to JSON files
    # save_data_to_json(courses, os.path.join(output_dir, f'{course_type}_courses.json'))
    # save_data_to_json(course_structure, os.path.join(output_dir, f'{course_type}_structure.json'))

import re
import docx
import json
import os

def extract_course_data(doc_path):
    doc = docx.Document(doc_path)
    courses = {}
    current_course_code = None
    current_unit = None

    # Updated regex to handle cases where course codes and names are separated by a colon, dash, or space.
    course_code_regex = r'^(MDS|MST|MDA)([0-9A-Za-z]+)\s*[:;\-]?\s*(.+)$'
    unit_regex = r'^(?:[Uu][Nn][Ii][Tt])\s+(\d+)\s*[:;\-]?\s*(.+)$'  # Case-insensitive 'UNIT'

    for para in doc.paragraphs:
        text = para.text.strip()

        # Extract course code and name
        course_code_match = re.match(course_code_regex, text)
        if course_code_match:
            current_course_code = course_code_match.group(1) + course_code_match.group(2)
            course_name = course_code_match.group(3).strip()
            courses[current_course_code] = {
                'course_name': course_name,
                'units': {}
            }
            current_unit = None
            continue

        # Extract unit number and name
        unit_match = re.match(unit_regex, text)
        if unit_match and current_course_code:
            unit_number = f"{current_course_code}_UNIT {unit_match.group(1)}"
            unit_name = unit_match.group(2).strip()
            courses[current_course_code]['units'][unit_number] = {
                'unit_name': unit_name,
                'subtopics': []
            }
            current_unit = unit_number
            continue

        # Extract subtopics
        if current_unit and current_course_code:
            subtopics = re.split(r'(?<!e\.g)\.|[,:;–—\-]', text)
            subtopics = [sub.strip() for sub in subtopics if sub.strip()]
            courses[current_course_code]['units'][current_unit]['subtopics'].extend(subtopics)

    return courses

def get_all_subtopics(courses):
    subtopics = [sub for course in courses.values() for unit in course['units'].values() for sub in unit['subtopics']]
    return sorted(set(subtopics))

def combine_all_courses(doc_paths):
    all_course_codes = []
    all_course_names = []
    all_unit_names = []
    all_subtopics = []
    combined_course_structure = {}

    for course_type, doc_path in doc_paths.items():
        courses = extract_course_data(doc_path)

        # Append course details
        all_course_codes.extend(courses.keys())
        all_course_names.extend([data['course_name'] for data in courses.values()])
        all_unit_names.extend([unit_data['unit_name'] for unit_data in [unit for course_data in courses.values() for unit in course_data['units'].values()]])
        all_subtopics.extend(get_all_subtopics(courses))

        # Adding course data to the combined structure
        combined_course_structure[course_type] = courses

    return (all_course_codes, all_course_names, all_unit_names, sorted(set(all_subtopics)), combined_course_structure)



# Define paths to syllabuses
doc_paths = {
    "MDS": '/content/MDS.docx',
    "MST": '/content/MST.docx',
    "MDA": '/content/MDA.docx'
}

# Process all syllabuses and combine data
all_course_codes, all_course_names, all_unit_names, all_subtopics, combined_course_structure = combine_all_courses(doc_paths)

# Print the combined data for verification
print("Combined Course Codes:")
print(len(all_course_codes), all_course_codes)
print("\nCombined Course Names:")
print(len(all_course_names), all_course_names)
print("\nCombined Unit Names:")
print(len(all_unit_names), all_unit_names)
print("\nCombined Subtopics:")
print(len(all_subtopics), all_subtopics)
print("\nCombined Course Structure:")
print(json.dumps(combined_course_structure, indent=4))

"""PREPROCESSING"""

import re
import docx
import json
import string
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from collections import defaultdict
import nltk

# Ensure you have downloaded necessary NLTK data
nltk.download('stopwords')
nltk.download('wordnet')

# Initialize preprocessing tools
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()  # For lemmatization

def preprocess_subtopic(subtopic, remove_stopwords=True, use_lemmatization=True):
    # Convert to lowercase
    subtopic = subtopic.lower()

    # Remove punctuation
    subtopic = subtopic.translate(str.maketrans('', '', string.punctuation))

    # # Remove numbers
    # subtopic = re.sub(r'\d+', '', subtopic)

    # Remove extra whitespace
    subtopic = ' '.join(subtopic.split())

    # Remove stopwords
    if remove_stopwords:
        words = subtopic.split()
        words = [word for word in words if word not in stop_words]
        subtopic = ' '.join(words)

    # Apply lemmatization
    if use_lemmatization:
        words = subtopic.split()
        words = [lemmatizer.lemmatize(word) for word in words]
        subtopic = ' '.join(words)

    return subtopic

def extract_course_data(doc_path):
    doc = docx.Document(doc_path)
    courses = {}
    current_course_code = None
    current_unit = None

    # Updated regex to handle cases where course codes and names are separated by a colon, dash, or space.
    course_code_regex = r'^(MDS|MST|MDA)([0-9A-Za-z]+)\s*[:;\-]?\s*(.+)$'
    unit_regex = r'^(?:[Uu][Nn][Ii][Tt])\s+(\d+)\s*[:;\-]?\s*(.+)$'  # Case-insensitive 'UNIT'

    for para in doc.paragraphs:
        text = para.text.strip()

        # Extract course code and name
        course_code_match = re.match(course_code_regex, text)
        if course_code_match:
            current_course_code = course_code_match.group(1) + course_code_match.group(2)
            course_name = course_code_match.group(3).strip()
            courses[current_course_code] = {
                'course_name': course_name,
                'units': {}
            }
            current_unit = None
            continue

        # Extract unit number and name
        unit_match = re.match(unit_regex, text)
        if unit_match and current_course_code:
            unit_number = f"{current_course_code}_UNIT {unit_match.group(1)}"
            unit_name = unit_match.group(2).strip()
            courses[current_course_code]['units'][unit_number] = {
                'unit_name': unit_name,
                'subtopics': []
            }
            current_unit = unit_number
            continue

        # Extract subtopics
        if current_unit and current_course_code:
            subtopics = re.split(r'(?<!e\.g)\.|[,:;–—\-]', text)
            subtopics = [sub.strip() for sub in subtopics if sub.strip()]
            processed_subtopics = [preprocess_subtopic(sub) for sub in subtopics]
            courses[current_course_code]['units'][current_unit]['subtopics'].extend(processed_subtopics)

    return courses

def get_all_subtopics(courses):
    subtopics = [sub for course in courses.values() for unit in course['units'].values() for sub in unit['subtopics']]
    return sorted(set(subtopics))

def combine_all_courses(doc_paths):
    all_course_codes = []
    all_course_names = []
    all_unit_names = []
    all_subtopics = []
    combined_course_structure = {}

    for course_type, doc_path in doc_paths.items():
        courses = extract_course_data(doc_path)

        # Append course details
        all_course_codes.extend(courses.keys())
        all_course_names.extend([data['course_name'] for data in courses.values()])
        all_unit_names.extend([unit_data['unit_name'] for unit_data in [unit for course_data in courses.values() for unit in course_data['units'].values()]])
        all_subtopics.extend(get_all_subtopics(courses))

        # Adding course data to the combined structure
        combined_course_structure[course_type] = courses

    return all_course_codes, all_course_names, all_unit_names, sorted(set(all_subtopics)), combined_course_structure

# def save_data_to_json(data, filename):
#     with open(filename, 'w') as json_file:
#         json.dump(data, json_file, indent=4)

# Define paths to syllabuses
doc_paths = {
    "MDS": '/content/MDS.docx',
    "MST": '/content/MST.docx',
    "MDA": '/content/MDA.docx'
}

# Process all syllabuses and combine data
all_course_codes, all_course_names, all_unit_names, all_subtopics, combined_course_structure = combine_all_courses(doc_paths)

# Print the combined data for verification
print("Combined Course Codes:")
print(len(all_course_codes), all_course_codes)
print("\nCombined Course Names:")
print(len(all_course_names), all_course_names)
print("\nCombined Unit Names:")
print(len(all_unit_names), all_unit_names)
print("\nCombined Subtopics:")
print(len(all_subtopics), all_subtopics)
print("\nCombined Course Structure:")
print(json.dumps(combined_course_structure, indent=4))

import networkx as nx
import matplotlib.pyplot as plt

def create_shell_graph(combined_course_structure):
    G = nx.Graph()

    shell_layers = [[], [], [], []]  # Four layers: Program, Course Code, Unit, Subunit

    # Add nodes and edges to the graph
    for program, courses in combined_course_structure.items():
        # Add program node
        G.add_node(program, color='green')
        shell_layers[0].append(program)

        for course_code, course_data in courses.items():
            # Add course code node
            G.add_node(course_code, color='orange')
            G.add_edge(program, course_code)
            shell_layers[1].append(course_code)

            for unit_number, unit_data in course_data['units'].items():
                # Add unit number node
                G.add_node(unit_number, color='yellow')
                G.add_edge(course_code, unit_number)
                shell_layers[2].append(unit_number)

                for subunit in unit_data['subtopics']:
                    # Add subunit node
                    subunit_node = f"{unit_number}: {subunit}"
                    G.add_node(subunit_node, color='lightblue')
                    G.add_edge(unit_number, subunit_node)
                    shell_layers[3].append(subunit_node)

    return G, shell_layers

def plot_shell_graph(G, shell_layers):
    pos = nx.shell_layout(G, nlist=shell_layers)

    # Get node attributes
    node_colors = [G.nodes[node].get('color', 'lightblue') for node in G.nodes]

    plt.figure(figsize=(15, 15))
    nx.draw(G, pos, with_labels=True, node_color=node_colors, node_size=800, edge_color='grey', font_size=5, font_color='black')
    plt.show()

# Assuming combined_course_structure is already created from the previous code

# Create and plot the shell graph
G, shell_layers = create_shell_graph(combined_course_structure)
plot_shell_graph(G, shell_layers)

"""SIMILARITY CHECKING

Collect all subunits: Extract subunits from your combined course structure.<br>
Calculate similarities: Use TF-IDF vectorization and cosine similarity to find similarities between subunits.<br>
Visualize similarities: Create a network graph to visualize these similarities.<br>
"""

def get_preprocessed_subunits(combined_course_structure):
    all_subunits = []
    subunit_lookup = {}  # To track which course and unit each subunit belongs to

    for program, courses in combined_course_structure.items():
        for course_code, course_data in courses.items():
            for unit_code, unit_data in course_data['units'].items():
                for subunit in unit_data['subtopics']:
                    all_subunits.append(subunit)
                    subunit_lookup[subunit] = f"{course_code} - {unit_code}"

    return all_subunits, subunit_lookup

# Collect preprocessed subunits
all_subunits, subunit_lookup = get_preprocessed_subunits(combined_course_structure)

from sklearn.feature_extraction.text import TfidfVectorizer # Import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity # Import cosine_similarity
def calculate_subunit_similarities(all_subunits, subunit_lookup):
    """Calculate similarities between subunits with more than 1 word and a similarity score between 0.7 and 0.99."""
    similarities = []

    valid_subunits = [
        subunit for subunit in all_subunits if len(subunit.split()) > 1
    ]

    if len(valid_subunits) < 2:
        return similarities

    # Compute the TF-IDF vectors for all valid subunits
    vectorizer = TfidfVectorizer().fit_transform(valid_subunits)
    cosine_sim_matrix = cosine_similarity(vectorizer)

    # A set to keep track of already processed pairs
    processed_pairs = set()

    for i in range(len(valid_subunits)):
        for j in range(i + 1, len(valid_subunits)):  # Ensure no self-comparison and no duplicate comparisons
            similarity = cosine_sim_matrix[i, j]
            if 0.7 < similarity < 0.99:  # Adjusted threshold for similarity
                subunit1 = valid_subunits[i]
                subunit2 = valid_subunits[j]

                if (subunit1, subunit2) not in processed_pairs and (subunit2, subunit1) not in processed_pairs:
                    source1 = subunit_lookup[subunit1]
                    source2 = subunit_lookup[subunit2]
                    similarities.append((subunit1, source1, subunit2, source2, similarity))
                    processed_pairs.add((subunit1, subunit2))

    return similarities

# Calculate similarities
similarities = calculate_subunit_similarities(all_subunits, subunit_lookup)

# Print the subunits being compared along with their similarity scores
print("Subunits and Their Similarity Scores:")
for subunit1, source1, subunit2, source2, similarity in similarities:
    print(f"'{subunit1}' from {source1} is similar to '{subunit2}' from {source2} with similarity score: {similarity:.2f}")

similarities

"""#UNIT VISUALISTION AND ANALYSIS BASED ON SIMILARITY OF SUBUNITS

REPRESENTING UNITS OF ALL PROGRAMMES AS NODES
"""

import networkx as nx
import matplotlib.pyplot as plt

def create_program_unit_graph(combined_course_structure):
    G = nx.Graph()

    # Define colors for each program
    program_colors = {'MDS': 'blue', 'MST': 'red', 'MDA': 'green'}

    # Add program nodes
    for program, courses in combined_course_structure.items():
        G.add_node(program, type='program', color=program_colors.get(program, 'gray'), size=1000)
        # Add unit nodes and edges
        for course_code, course_data in courses.items():
            for unit_code, unit_data in course_data['units'].items():
                G.add_node(unit_code, type='unit', size=1000)
                # Connect units to their program
                G.add_edge(program, unit_code)

    return G

def color_subunits_by_program(G):
    # Assign colors to subunits based on their program connections
    color_map = {}
    for node in G.nodes:
        if G.nodes[node]['type'] == 'unit':
            connected_programs = [n for n in G.neighbors(node) if G.nodes[n]['type'] == 'program']
            if connected_programs:
                # Use the color of the connected program for the subunit
                color_map[node] = G.nodes[connected_programs[0]]['color']
            else:
                color_map[node] = 'lightgrey'  # Default color
    return color_map

# Create the graph from combined course structure
G = create_program_unit_graph(combined_course_structure)

# Get the color mapping for subunits
subunit_colors = color_subunits_by_program(G)

# Adjust layout parameters for better spacing
pos = nx.spring_layout(G, seed=42, k=0.5, iterations=100)  # Increased k and iterations

# Draw the graph
node_colors = [G.nodes[node].get('color', 'lightgrey') for node in G.nodes]
node_sizes = [G.nodes[node]['size'] for node in G.nodes]

plt.figure(figsize=(20, 20))  # Adjust figure size as needed
nx.draw_networkx_nodes(G, pos, node_color=[subunit_colors.get(node, node_colors[i]) for i, node in enumerate(G.nodes)], node_size=node_sizes, alpha=0.8)
nx.draw_networkx_edges(G, pos, edge_color='gray')

# Draw labels for all nodes with white background
labels = {node: node for node in G.nodes}
nx.draw_networkx_labels(G, pos, labels=labels, font_size=10, font_weight='light', bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))

plt.title("Program and Unit Nodes Graph")
plt.show()

""" CONNECTING UNITS IF SUBUNITS HAVE COSINE SIMILARITY(RED EDGE)"""

import networkx as nx
import matplotlib.pyplot as plt

def create_weighted_graph(similarities, combined_course_structure):
    G = nx.Graph()

    # Define colors for each program
    program_colors = {'MDS': 'blue', 'MST': 'red', 'MDA': 'green'}

    # Add program nodes
    for program, courses in combined_course_structure.items():
        G.add_node(program, type='program', color=program_colors.get(program, 'gray'), size=2000)
        # Add unit nodes and edges
        for course_code, course_data in courses.items():
            for unit_code, unit_data in course_data['units'].items():
                G.add_node(unit_code, type='unit', color=program_colors.get(program, 'lightgrey'), size=1500)
                # Connect units to their program
                G.add_edge(program, unit_code, color='gray', weight=1)

    # Dictionary to keep track of edge weights
    edge_weights = {}

    # Process similarities
    for subunit1, source1, subunit2, source2, similarity in similarities:
        unit1 = source1.split(' - ')[1]
        unit2 = source2.split(' - ')[1]

        if unit1 != unit2:
            # Handle edges between different units
            edge = (unit1, unit2) if unit1 < unit2 else (unit2, unit1)
            if edge in edge_weights:
                edge_weights[edge] += 1  # Increment weight if edge already exists
            else:
                edge_weights[edge] = 1  # Initialize weight if edge does not exist

    # Add weighted edges to the graph
    for (u, v), weight in edge_weights.items():
        if G.has_edge(u, v):
            # Update existing edge weight
            G[u][v]['weight'] = weight
            G[u][v]['color'] = 'red'
        else:
            # Add new edge with weight
            G.add_edge(u, v, weight=weight, color='red')

    return G

def color_subunits_by_program(G):
    color_map = {}
    for node in G.nodes:
        if G.nodes[node]['type'] == 'unit':
            # Get connected program nodes
            connected_programs = [n for n in G.neighbors(node) if G.nodes[n]['type'] == 'program']
            if connected_programs:
                color_map[node] = G.nodes[connected_programs[0]]['color']
            else:
                color_map[node] = 'lightgrey'  # Default color if no connected program
    return color_map



# Create the weighted graph
G = create_weighted_graph(similarities, combined_course_structure)

# Get the color mapping for subunits
subunit_colors = color_subunits_by_program(G)

# Adjust layout parameters for better spacing
pos = nx.spring_layout(G, seed=42, k=0.5, iterations=100)  # Increased k and iterations

# Normalize edge weights for better visualization
weights = [G[u][v]['weight'] for u, v in G.edges()]
max_weight = max(weights) if weights else 1
edge_widths = [1 for _ in G.edges()]  # Constant edge width

# Draw the graph
node_colors = [G.nodes[node].get('color', 'lightgrey') for node in G.nodes]
node_sizes = [G.nodes[node].get('size', 1500) for node in G.nodes]
edge_colors = [G[u][v].get('color', 'gray') for u, v in G.edges()]

plt.figure(figsize=(20, 20))  # Adjust figure size as needed
nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes, alpha=0.8)
nx.draw_networkx_edges(G, pos, edge_color=edge_colors, width=edge_widths)

# Draw edge labels
edge_labels = {(u, v): f"{G[u][v]['weight']}" for u, v in G.edges()}
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='black', font_size=10)

nx.draw_networkx_labels(G, pos, font_size=8, font_weight='light', bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))

plt.title("Program and Unit Nodes Graph with Weighted Edges and Color-Coded Similarities")
plt.show()

"""GRAPHICAL REPRESENTATION IN KAMADA-KAWAI LAYOUT"""

import networkx as nx
import matplotlib.pyplot as plt

def create_weighted_graph(similarities, combined_course_structure):
    G = nx.Graph()

    # Define colors for each program
    program_colors = {'MDS': 'blue', 'MST': 'red', 'MDA': 'green'}

    # Add program nodes
    for program, courses in combined_course_structure.items():
        G.add_node(program, type='program', color=program_colors.get(program, 'gray'), size=2000)
        # Add unit nodes and edges
        for course_code, course_data in courses.items():
            for unit_code, unit_data in course_data['units'].items():
                G.add_node(unit_code, type='unit', color=program_colors.get(program, 'lightgrey'), size=1500)
                # Connect units to their program
                G.add_edge(program, unit_code, color='gray', weight=1)

    # Dictionary to keep track of edge weights
    edge_weights = {}

    # Process similarities
    for subunit1, source1, subunit2, source2, similarity in similarities:
        unit1 = source1.split(' - ')[1]
        unit2 = source2.split(' - ')[1]

        if unit1 != unit2:
            # Handle edges between different units
            edge = (unit1, unit2) if unit1 < unit2 else (unit2, unit1)
            if edge in edge_weights:
                edge_weights[edge] += 1  # Increment weight if edge already exists
            else:
                edge_weights[edge] = 1  # Initialize weight if edge does not exist

    # Add weighted edges to the graph
    for (u, v), weight in edge_weights.items():
        if G.has_edge(u, v):
            # Update existing edge weight
            G[u][v]['weight'] = weight
            G[u][v]['color'] = 'red'
        else:
            # Add new edge with weight
            G.add_edge(u, v, weight=weight, color='red')

    return G

def color_subunits_by_program(G):
    color_map = {}
    for node in G.nodes:
        if G.nodes[node]['type'] == 'unit':
            # Get connected program nodes
            connected_programs = [n for n in G.neighbors(node) if G.nodes[n]['type'] == 'program']
            if connected_programs:
                color_map[node] = G.nodes[connected_programs[0]]['color']
            else:
                color_map[node] = 'lightgrey'  # Default color if no connected program
    return color_map

# Create the weighted graph
G = create_weighted_graph(similarities, combined_course_structure)

# Get the color mapping for subunits
subunit_colors = color_subunits_by_program(G)

# Kamada-Kaway Layout
pos_kamada_kaway = nx.kamada_kawai_layout(G)



# # Shell Layout
# shells = [list(G.nodes)]
# pos_shell = nx.shell_layout(G, nlist=shells)

# Adjust edge widths based on weights
weights = [G[u][v]['weight'] for u, v in G.edges()]
max_weight = max(weights) if weights else 1
edge_widths = [G[u][v]['weight'] / max_weight * 4 for u, v in G.edges()]  # Scale width for visibility

def draw_layout(pos, layout_name):
    plt.figure(figsize=(20, 20))  # Adjust figure size as needed
    node_colors = [G.nodes[node].get('color', 'lightgrey') for node in G.nodes]
    node_sizes = [G.nodes[node].get('size', 1500) for node in G.nodes]
    edge_colors = [G[u][v].get('color', 'gray') for u, v in G.edges()]

    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes, alpha=0.8)
    nx.draw_networkx_edges(G, pos, edge_color=edge_colors, width=edge_widths)
    edge_labels = {(u, v): f"{G[u][v]['weight']}" for u, v in G.edges()}
    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='black', font_size=10)
    nx.draw_networkx_labels(G, pos, font_size=8, font_weight='light', bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))
    plt.title(f"Graph Layout: {layout_name}")
    plt.show()

# Draw graphs with different layouts
draw_layout(pos_kamada_kaway, "Kamada-Kawai Layout")

# Print edge weights and connected nodes
print("Edge Weights and Connected Nodes:")
for u, v in G.edges():
        weight = G[u][v]['weight']
        print(f"Edge between '{u}' and '{v}' has weight: {weight}")

"""ANALYSING DIFFERENT CENTRALITY MEASURES

1. Degree Centrality: Measures the number of direct connections a node has. Nodes with high degree centrality are often considered important because they are highly connected.<BR>

2. Closeness Centrality: Measures how close a node is to all other nodes in the network. Nodes with high closeness centrality can spread information quickly across the network.<BR>

3. Betweenness Centrality: Measures how often a node acts as a bridge along the shortest path between two other nodes. Nodes with high betweenness centrality are important for maintaining network connectivity.<BR>

4. Eigenvector Centrality: Measures a node's influence based on the influence of its neighbors. Nodes with high eigenvector centrality are well-connected to other well-connected nodes.<BR>
"""

import networkx as nx
import matplotlib.pyplot as plt

def compute_centralities(G):
    # Compute centrality measures
    degree_centrality = nx.degree_centrality(G)
    closeness_centrality = nx.closeness_centrality(G)
    betweenness_centrality = nx.betweenness_centrality(G)
    eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)

    return {
        'degree': degree_centrality,
        'closeness': closeness_centrality,
        'betweenness': betweenness_centrality,
        'eigenvector': eigenvector_centrality
    }

def print_centrality_interpretations(centralities):
    print("Centrality Measures Interpretation:")

    # Degree Centrality
    print("\nDegree Centrality:")
    for node, value in sorted(centralities['degree'].items(), key=lambda x: x[1], reverse=True):
        print(f"Node: {node}, Degree Centrality: {value:.4f}")

    # Closeness Centrality
    print("\nCloseness Centrality:")
    for node, value in sorted(centralities['closeness'].items(), key=lambda x: x[1], reverse=True):
        print(f"Node: {node}, Closeness Centrality: {value:.4f}")

    # Betweenness Centrality
    print("\nBetweenness Centrality:")
    for node, value in sorted(centralities['betweenness'].items(), key=lambda x: x[1], reverse=True):
        print(f"Node: {node}, Betweenness Centrality: {value:.4f}")

    # Eigenvector Centrality
    print("\nEigenvector Centrality:")
    for node, value in sorted(centralities['eigenvector'].items(), key=lambda x: x[1], reverse=True):
        print(f"Node: {node}, Eigenvector Centrality: {value:.4f}")

# Create the weighted graph
G = create_weighted_graph(similarities, combined_course_structure)

# Compute centrality measures
centralities = compute_centralities(G)

# Print interpretations of centrality measures
print_centrality_interpretations(centralities)

"""TOP 5 IMPORTANT NODES BASED ON ALL CENTRALITY MEASURES"""

import networkx as nx

def print_top_centrality_nodes(G):
    # Compute centrality measures
    degree_centrality = nx.degree_centrality(G)
    betweenness_centrality = nx.betweenness_centrality(G)
    closeness_centrality = nx.closeness_centrality(G)
    eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)

    # Sort nodes by centrality measures and get the top 5 nodes
    top_5_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]
    top_5_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]
    top_5_closeness = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]
    top_5_eigenvector = sorted(eigenvector_centrality.items(), key=lambda x: x[1], reverse=True)[:5]

    # Print the top 5 nodes for each centrality measure
    print("Top 5 nodes by Degree Centrality:")
    for node, centrality in top_5_degree:
        print(f"{node}: {centrality:.4f}")

    print("\nTop 5 nodes by Betweenness Centrality:")
    for node, centrality in top_5_betweenness:
        print(f"{node}: {centrality:.4f}")

    print("\nTop 5 nodes by Closeness Centrality:")
    for node, centrality in top_5_closeness:
        print(f"{node}: {centrality:.4f}")

    print("\nTop 5 nodes by Eigenvector Centrality:")
    for node, centrality in top_5_eigenvector:
        print(f"{node}: {centrality:.4f}")


print_top_centrality_nodes(G)

"""REMOVING PROGRAM NODES TO SEE HOW CENTRALITY / IMPORTANT NODES CHANGE"""

import networkx as nx
import matplotlib.pyplot as plt

def create_graph_A(H):
    # Initialize new graph A
    A = nx.Graph()

    # Add nodes and edges from H to A, excluding program nodes
    for node in H.nodes:
        if 'type' in H.nodes[node] and H.nodes[node]['type'] == 'unit':
            # Add unit node to graph A
            A.add_node(node, **H.nodes[node])

    # Add edges between units (if both nodes are units)
    for u, v in H.edges():
        if 'type' in H.nodes[u] and H.nodes[u]['type'] == 'unit' and 'type' in H.nodes[v] and H.nodes[v]['type'] == 'unit':
            # Add edge to graph A
            A.add_edge(u, v, **H[u][v])

    return A

# Create graph A by removing program nodes from graph H
A = create_graph_A(H)

# Adjust layout parameters for better spacing
pos = nx.spring_layout(A, seed=42, k=0.5, iterations=100)  # Increased k and iterations

# Normalize edge weights for better visualization
weights = [A[u][v]['weight'] for u, v in A.edges()]
max_weight = max(weights) if weights else 1
edge_widths = [weight / max_weight * 5 for weight in weights]  # Normalize and scale edge widths

# Draw the graph
node_colors = [A.nodes[node].get('color', 'lightgrey') for node in A.nodes]
node_sizes = [A.nodes[node].get('size', 1500) for node in A.nodes]
edge_colors = [A[u][v].get('color', 'gray') for u, v in A.edges()]

plt.figure(figsize=(20, 20))  # Adjust figure size as needed
nx.draw_networkx_nodes(A, pos, node_color=node_colors, node_size=node_sizes, alpha=0.8)
nx.draw_networkx_edges(A, pos, edge_color=edge_colors, width=edge_widths)

# Draw edge labels
edge_labels = {(u, v): f"{A[u][v]['weight']}" for u, v in A.edges()}
nx.draw_networkx_edge_labels(A, pos, edge_labels=edge_labels, font_color='black', font_size=10)

nx.draw_networkx_labels(A, pos, font_size=8, font_weight='light', bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))

plt.title("Graph  - Unit Nodes Only with Weighted Edges")
plt.show()

import networkx as nx

# Assume A is your graph
# Compute centrality measures for graph A

# Degree Centrality
degree_centrality = nx.degree_centrality(A)
top_degree_centrality = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]
print("Top 5 nodes by Degree Centrality:", top_degree_centrality)

# Betweenness Centrality
betweenness_centrality = nx.betweenness_centrality(A)
top_betweenness_centrality = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]
print("Top 5 nodes by Betweenness Centrality:", top_betweenness_centrality)

# Closeness Centrality
closeness_centrality = nx.closeness_centrality(A)
top_closeness_centrality = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]
print("Top 5 nodes by Closeness Centrality:", top_closeness_centrality)

# Eigenvector Centrality
eigenvector_centrality = nx.eigenvector_centrality(A, max_iter=1000)
top_eigenvector_centrality = sorted(eigenvector_centrality.items(), key=lambda x: x[1], reverse=True)[:5]
print("Top 5 nodes by Eigenvector Centrality:", top_eigenvector_centrality)

# PageRank
pagerank = nx.pagerank(A)
top_pagerank = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:5]
print("Top 5 nodes by PageRank:", top_pagerank)

"""COMMUNITY DETECTION"""

pip install python-louvain

"""LOUVAIN METHOD"""

import networkx as nx
import matplotlib.pyplot as plt
import community.community_louvain as community_louvain  # Ensure this library is installed

def louvain_community_detection(G):
    # Apply Louvain community detection
    partition = community_louvain.best_partition(G)

    # Create a color map based on community partition
    unique_communities = set(partition.values())
    colors = plt.cm.get_cmap('tab20', len(unique_communities))
    node_colors = [colors(partition[node]) for node in G.nodes]

    # Draw the graph with community colors
    pos = nx.spring_layout(G, seed=42, k=0.5, iterations=100)
    plt.figure(figsize=(20, 20))
    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=1500, alpha=0.8)
    nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5)

    # Draw edge labels
    edge_labels = {(u, v): f"{G[u][v]['weight']}" for u, v in G.edges()}
    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='black', font_size=10)

    # Draw node labels
    nx.draw_networkx_labels(G, pos, font_size=8, font_weight='light', bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))

    plt.title("Community Detection using Louvain Method")
    plt.show()

    return partition, colors

def visualize_communities(G, partition, colors):
    # Extract and visualize each community individually
    unique_communities = set(partition.values())
    pos = nx.spring_layout(G, seed=42, k=0.5, iterations=100)

    for community_id in unique_communities:
        # Extract nodes in this community
        community_nodes = [node for node, comm_id in partition.items() if comm_id == community_id]

        # Create subgraph for this community
        subgraph = G.subgraph(community_nodes)

        # Generate a layout for this subgraph
        subgraph_pos = nx.spring_layout(subgraph, seed=42, k=0.5, iterations=100)

        plt.figure(figsize=(12, 12))
        nx.draw_networkx_nodes(subgraph, subgraph_pos, node_size=1500, alpha=0.8, node_color=[colors(community_id)])
        nx.draw_networkx_edges(subgraph, subgraph_pos, width=1.0, alpha=0.5)

        # Draw edge labels
        edge_labels = {(u, v): f"{subgraph[u][v]['weight']}" for u, v in subgraph.edges()}
        nx.draw_networkx_edge_labels(subgraph, subgraph_pos, edge_labels=edge_labels, font_color='black', font_size=10)

        # Draw node labels
        nx.draw_networkx_labels(subgraph, subgraph_pos, font_size=8, font_weight='light', bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))

        plt.title(f"Community {community_id} Visualization")
        plt.show()

# Apply Louvain community detection and visualize
louvain_partition, colors = louvain_community_detection(G)

# Visualize each community separately
visualize_communities(G, louvain_partition, colors)

"""GIRVAN-NEWMAN"""

import networkx as nx
import matplotlib.pyplot as plt
from networkx.algorithms.community import girvan_newman
import matplotlib.colors as mcolors

def girvan_newman_community_detection(G):
    # Apply Girvan-Newman community detection
    comp = girvan_newman(G)
    # Get the first level of communities
    communities = next(comp)

    # Create a mapping of nodes to community indices
    community_mapping = {}
    for idx, community in enumerate(communities):
        for node in community:
            community_mapping[node] = idx

    # Use very contrasting colors from a predefined set
    contrast_colors = list(mcolors.TABLEAU_COLORS.values()) + list(mcolors.CSS4_COLORS.values())

    # Limit the number of colors to the number of unique communities
    unique_communities = set(community_mapping.values())
    colors = contrast_colors[:len(unique_communities)]

    # Assign colors to nodes based on their community
    node_colors = [colors[community_mapping[node]] for node in G.nodes]

    # Draw the graph with community colors
    pos = nx.spring_layout(G, seed=42, k=0.5, iterations=100)
    plt.figure(figsize=(20, 20))
    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=1500, alpha=0.8)
    nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5)

    # Draw edge labels
    edge_labels = {(u, v): f"{G[u][v]['weight']}" for u, v in G.edges()}
    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='black', font_size=10)

    # Draw node labels
    nx.draw_networkx_labels(G, pos, font_size=8, font_weight='light', bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))

    plt.title("Community Detection using Girvan-Newman Method with Contrasting Colors")
    plt.show()

    return community_mapping

# Example usage
# Assuming G is your graph
girvan_newman_mapping = girvan_newman_community_detection(G)

"""SPECTRAL CLUSTERING"""

import networkx as nx
import matplotlib.pyplot as plt
from sklearn.cluster import SpectralClustering
import numpy as np

def spectral_clustering_community_detection(G, n_clusters=4):
    # Convert the graph to an adjacency matrix
    adjacency_matrix = nx.to_numpy_array(G)

    # Apply Spectral Clustering
    sc = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', assign_labels='kmeans')
    labels = sc.fit_predict(adjacency_matrix)

    # Assign clusters to nodes
    partition = {node: labels[i] for i, node in enumerate(G.nodes())}

    # Define a set of distinct colors
    distinct_colors = ['#FF5733', '#33FF57', '#3357FF', '#FF33A8', '#33FFF5', '#F5FF33', '#FF8C33', '#8C33FF', '#33FF8C', '#FF3357']

    # Map colors to communities
    unique_communities = set(partition.values())
    community_colors = {comm_id: distinct_colors[i % len(distinct_colors)] for i, comm_id in enumerate(unique_communities)}
    node_colors = [community_colors[partition[node]] for node in G.nodes]

    # Draw the graph with community colors
    pos = nx.spring_layout(G, seed=42, k=0.5, iterations=100)
    plt.figure(figsize=(20, 20))
    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=1500, alpha=0.8)
    nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5)

    # Draw edge labels
    edge_labels = {(u, v): f"{G[u][v]['weight']}" for u, v in G.edges()}
    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='black', font_size=10)

    # Draw node labels
    nx.draw_networkx_labels(G, pos, font_size=8, font_weight='light', bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))

    plt.title("Community Detection using Spectral Clustering")
    plt.show()

    return partition, community_colors

def visualize_spectral_communities(G, partition, community_colors):
    # Extract and visualize each community individually
    unique_communities = set(partition.values())
    pos = nx.spring_layout(G, seed=42, k=0.5, iterations=100)

    for community_id in unique_communities:
        # Extract nodes in this community
        community_nodes = [node for node, comm_id in partition.items() if comm_id == community_id]

        # Create subgraph for this community
        subgraph = G.subgraph(community_nodes)

        # Generate a layout for this subgraph
        subgraph_pos = nx.spring_layout(subgraph, seed=42, k=0.5, iterations=100)

        plt.figure(figsize=(12, 12))
        nx.draw_networkx_nodes(subgraph, subgraph_pos, node_size=1500, alpha=0.8, node_color=[community_colors[community_id]])
        nx.draw_networkx_edges(subgraph, subgraph_pos, width=1.0, alpha=0.5)

        # Draw edge labels
        edge_labels = {(u, v): f"{subgraph[u][v]['weight']}" for u, v in subgraph.edges()}
        nx.draw_networkx_edge_labels(subgraph, subgraph_pos, edge_labels=edge_labels, font_color='black', font_size=10)

        # Draw node labels
        nx.draw_networkx_labels(subgraph, subgraph_pos, font_size=8, font_weight='light', bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))

        plt.title(f"Spectral Community {community_id} Visualization")
        plt.show()

# Apply Spectral Clustering community detection and visualize
spectral_partition, community_colors = spectral_clustering_community_detection(G)

# Visualize each community separately
visualize_spectral_communities(G, spectral_partition, community_colors)

"""COMPARISON OF THREE METHODS"""

import networkx as nx
import matplotlib.pyplot as plt
from sklearn.cluster import SpectralClustering
import community.community_louvain as community_louvain
from networkx.algorithms.community import girvan_newman
from networkx.algorithms.community.quality import modularity
from sklearn.metrics import silhouette_score
import numpy as np
import itertools

def louvain_community_detection(G):
    partition = community_louvain.best_partition(G)
    return partition

def girvan_newman_community_detection(G, k=4):
    communities_generator = girvan_newman(G)
    for communities in itertools.islice(communities_generator, k-1):
        pass
    partition = {node: i for i, community in enumerate(communities) for node in community}
    return partition

def spectral_clustering_community_detection(G, n_clusters=4):
    adjacency_matrix = nx.to_numpy_array(G)
    sc = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', assign_labels='kmeans')
    labels = sc.fit_predict(adjacency_matrix)
    partition = {node: labels[i] for i, node in enumerate(G.nodes())}
    return partition

def evaluate_clustering(G, partition):
    modularity_score = modularity(G, [set([node for node, comm_id in partition.items() if comm_id == i]) for i in set(partition.values())])
    labels = np.array([partition[node] for node in G.nodes()])
    adjacency_matrix = nx.to_numpy_array(G)
    silhouette_avg = silhouette_score(adjacency_matrix, labels, metric='precomputed')
    return modularity_score, silhouette_avg

def visualize_communities(G, partition, title="Community Visualization"):
    unique_communities = set(partition.values())
    colors = plt.cm.get_cmap('tab20', len(unique_communities))
    node_colors = [colors(partition[node]) for node in G.nodes]

    pos = nx.spring_layout(G, seed=42, k=0.5, iterations=100)
    plt.figure(figsize=(12, 12))
    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=1500, alpha=0.8)
    nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5)
    nx.draw_networkx_labels(G, pos, font_size=8, font_weight='light', bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))
    plt.title(title)
    plt.show()

# Apply clustering methods
louvain_partition = louvain_community_detection(G)
girvan_newman_partition = girvan_newman_community_detection(G)
spectral_partition = spectral_clustering_community_detection(G)

# Evaluate clustering methods
louvain_modularity, louvain_silhouette = evaluate_clustering(G, louvain_partition)
girvan_newman_modularity, girvan_newman_silhouette = evaluate_clustering(G, girvan_newman_partition)
spectral_modularity, spectral_silhouette = evaluate_clustering(G, spectral_partition)

# Print evaluation results
print("Louvain Method:")
print(f"Modularity: {louvain_modularity}, Silhouette Score: {louvain_silhouette}\n")

print("Girvan-Newman Method:")
print(f"Modularity: {girvan_newman_modularity}, Silhouette Score: {girvan_newman_silhouette}\n")

print("Spectral Clustering Method:")
print(f"Modularity: {spectral_modularity}, Silhouette Score: {spectral_silhouette}\n")

# Visualize communities
visualize_communities(G, louvain_partition, title="Louvain Community Detection")
visualize_communities(G, girvan_newman_partition, title="Girvan-Newman Community Detection")
visualize_communities(G, spectral_partition, title="Spectral Clustering Community Detection")

"""SUBGRAPH (EACH PROGRAM)"""

import networkx as nx
import matplotlib.pyplot as plt

def create_weighted_graph(similarities, combined_course_structure):
    G = nx.Graph()

    # Define colors for each program
    program_colors = {'MDS': 'blue', 'MST': 'red', 'MDA': 'green'}

    # Add program nodes
    for program, courses in combined_course_structure.items():
        G.add_node(program, type='program', color=program_colors.get(program, 'gray'), size=2000)
        # Add unit nodes and edges
        for course_code, course_data in courses.items():
            for unit_code, unit_data in course_data['units'].items():
                G.add_node(unit_code, type='unit', color=program_colors.get(program, 'lightgrey'), size=1500)
                # Connect units to their program
                G.add_edge(program, unit_code, color='gray', weight=1)

    # Dictionary to keep track of edge weights
    edge_weights = {}

    # Process similarities
    for subunit1, source1, subunit2, source2, similarity in similarities:
        unit1 = source1.split(' - ')[1]
        unit2 = source2.split(' - ')[1]

        if unit1 != unit2:
            # Handle edges between different units
            edge = (unit1, unit2) if unit1 < unit2 else (unit2, unit1)
            if edge in edge_weights:
                edge_weights[edge] += 1  # Increment weight if edge already exists
            else:
                edge_weights[edge] = 1  # Initialize weight if edge does not exist

    # Add weighted edges to the graph
    for (u, v), weight in edge_weights.items():
        if G.has_edge(u, v):
            # Update existing edge weight
            G[u][v]['weight'] = weight
            G[u][v]['color'] = 'red'
        else:
            # Add new edge with weight
            G.add_edge(u, v, weight=weight, color='red')

    return G

def color_subunits_by_program(G):
    color_map = {}
    for node in G.nodes:
        if G.nodes[node]['type'] == 'unit':
            # Get connected program nodes
            connected_programs = [n for n in G.neighbors(node) if G.nodes[n]['type'] == 'program']
            if connected_programs:
                color_map[node] = G.nodes[connected_programs[0]]['color']
            else:
                color_map[node] = 'lightgrey'  # Default color if no connected program
    return color_map

def plot_subgraph(subgraph, title):
    pos = nx.spring_layout(subgraph, seed=42, k=0.5, iterations=100)
    plt.figure(figsize=(10, 10))
    node_colors = [subgraph.nodes[node].get('color', 'lightgrey') for node in subgraph.nodes]
    node_sizes = [subgraph.nodes[node].get('size', 1500) for node in subgraph.nodes]
    edge_colors = [subgraph[u][v].get('color', 'gray') for u, v in subgraph.edges()]

    nx.draw_networkx_nodes(subgraph, pos, node_color=node_colors, node_size=node_sizes, alpha=0.8)
    nx.draw_networkx_edges(subgraph, pos, edge_color=edge_colors, width=1)

    # Safely handle edge labels
    edge_labels = {(u, v): f"{subgraph[u][v].get('weight', 1)}" for u, v in subgraph.edges()}
    nx.draw_networkx_edge_labels(subgraph, pos, edge_labels=edge_labels, font_color='black', font_size=10)
    nx.draw_networkx_labels(subgraph, pos, font_size=8, font_weight='light', bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))

    plt.title(title)
    plt.show()

# Create the weighted graph
G = create_weighted_graph(similarities, combined_course_structure)

# Create and plot subgraphs

# Subgraphs for each program
programs = ['MDS', 'MST', 'MDA']
program_subgraphs = create_program_subgraphs(G, programs)
for program, subgraph in program_subgraphs.items():
    plot_subgraph(subgraph, f"Subgraph for Program {program}")

# Subgraph with high similarity scores
high_similarity_subgraph = create_high_similarity_subgraph(G, similarities, threshold=0.7)
plot_subgraph(high_similarity_subgraph, "Subgraph with High Similarity Scores")

# Subgraph for a specific unit
unit_of_interest = 'UNIT 1'  # Replace with your unit of interest
specific_unit_subgraph = create_specific_unit_subgraph(G, unit_of_interest)
plot_subgraph(specific_unit_subgraph, f"Subgraph for Unit {unit_of_interest}")

"""SUBGRAPH WITH SUBUNITS OF HIGH SIMILARITY SCORES"""

import networkx as nx
import matplotlib.pyplot as plt

def create_high_similarity_subgraph(G, similarities, threshold=0.7):
    # Identify high similarity edges, excluding self-loops
    high_similarity_edges = [
        (source1.split(' - ')[1], source2.split(' - ')[1])
        for subunit1, source1, subunit2, source2, similarity in similarities
        if similarity >= threshold and source1 != source2
    ]

    # Create the high similarity subgraph without self-loops
    high_similarity_subgraph = G.edge_subgraph(high_similarity_edges).copy()

    return high_similarity_subgraph

# Generate subgraph with high similarity edges
high_similarity_subgraph = create_high_similarity_subgraph(G, similarities, threshold=0.7)

# Plot the high similarity subgraph
pos = nx.spring_layout(high_similarity_subgraph, seed=42, k=0.5, iterations=100)
plt.figure(figsize=(15, 15))
node_colors = [high_similarity_subgraph.nodes[node].get('color', 'lightgrey') for node in high_similarity_subgraph.nodes]
node_sizes = [high_similarity_subgraph.nodes[node].get('size', 1500) for node in high_similarity_subgraph.nodes]
edge_colors = [high_similarity_subgraph[u][v].get('color', 'red') for u, v in high_similarity_subgraph.edges()]

nx.draw_networkx_nodes(high_similarity_subgraph, pos, node_color=node_colors, node_size=node_sizes, alpha=0.8)
nx.draw_networkx_edges(high_similarity_subgraph, pos, edge_color=edge_colors, width=2)

# Draw edge labels with similarity scores
edge_labels = {(u, v): f"{high_similarity_subgraph[u][v]['weight']}" for u, v in high_similarity_subgraph.edges()}
nx.draw_networkx_edge_labels(high_similarity_subgraph, pos, edge_labels=edge_labels, font_color='black', font_size=10)
nx.draw_networkx_labels(high_similarity_subgraph, pos, font_size=8, font_weight='light', bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))

plt.title("Subgraph with High Similarity Scores")
plt.show()

"""SUBGRAPH OF EACH UNIT NODE"""

def compute_edge_weights(similarities):
    edge_weights = {}

    for subunit1, source1, subunit2, source2, similarity in similarities:
        unit1 = source1.split(' - ')[1]
        unit2 = source2.split(' - ')[1]

        if unit1 != unit2:
            # Handle edges between different units
            edge = (unit1, unit2) if unit1 < unit2 else (unit2, unit1)
            if edge in edge_weights:
                edge_weights[edge] += 1  # Increment weight if edge already exists
            else:
                edge_weights[edge] = 1  # Initialize weight if edge does not exist

    return edge_weights

def create_similarity_subgraphs(G, edge_weights, similarity_threshold=0.8):
    # Create a dictionary to store subgraphs based on similarity
    similarity_subgraphs = {}

    # Create a subgraph for each pair of units with similarity above the threshold
    for (unit1, unit2), weight in edge_weights.items():
        if weight >= similarity_threshold:
            nodes = {unit1, unit2}
            edges = [(unit1, unit2)]

            # Create a subgraph for units with high similarity
            subgraph = G.subgraph(nodes).copy()
            subgraph.add_edges_from(edges)

            similarity_subgraphs[f"{unit1}-{unit2}"] = subgraph

    return similarity_subgraphs

# Compute edge weights from similarities
edge_weights = compute_edge_weights(similarities)

# Create and visualize additional subgraphs
unit_subgraphs = create_unit_subgraphs(G)
course_subgraphs = create_course_subgraphs(G, combined_course_structure)
similarity_subgraphs = create_similarity_subgraphs(G, edge_weights)

visualize_subgraphs(unit_subgraphs)
visualize_subgraphs(course_subgraphs)
visualize_subgraphs(similarity_subgraphs)

"""#SUBUNIT ANALYSIS

SUBUNIT GRAPH
"""

import networkx as nx
import matplotlib.pyplot as plt

def plot_subunit_similarity_graph(similarities):
    # Initialize a graph
    G = nx.Graph()

    # Add nodes and edges to the graph based on the similarity results
    for subunit1, source1, subunit2, source2, similarity in similarities:
        G.add_node(subunit1)
        G.add_node(subunit2)
        G.add_edge(subunit1, subunit2, weight=similarity)

    # Define positions for all nodes in the graph using spring layout
    pos = nx.spring_layout(G, seed=42)  # seed for reproducibility

    # Increase the figure size for a bigger graph
    plt.figure(figsize=(20, 20))

    # Draw the nodes with large size
    nx.draw_networkx_nodes(G, pos, node_size=1500, node_color='skyblue', alpha=0.8)

    # Draw the edges in red if they represent a similarity connection
    edges = nx.get_edge_attributes(G, 'weight')
    nx.draw_networkx_edges(G, pos, edgelist=edges.keys(),
                           width=[v * 3 for v in edges.values()],
                           alpha=0.7, edge_color='red')

    # Draw node labels with small font size
    nx.draw_networkx_labels(G, pos, font_size=6, font_color='black')

    # Remove axis for clarity
    plt.axis('off')

    # Show the graph
    plt.title('Subunit Similarity Graph', fontsize=16)
    plt.show()

# Plot the graph based on the calculated similarities
plot_subunit_similarity_graph(similarities)

"""PERFORM NODE EMBEDDINGS"""

pip install node2vec

from node2vec import Node2Vec
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

def perform_node2vec_embedding(G, dimensions=64, walk_length=30, num_walks=200, workers=4):
    # Create a Node2Vec model
    node2vec = Node2Vec(G, dimensions=dimensions, walk_length=walk_length, num_walks=num_walks, workers=workers)

    # Fit the model to the graph
    model = node2vec.fit(window=10, min_count=1, batch_words=4)

    # Get the embeddings for each node
    embeddings = {node: model.wv[node] for node in G.nodes()}

    return embeddings

# Perform node embedding on the graph
embeddings = perform_node2vec_embedding(G)

# Convert embeddings to a numpy array for analysis
embedding_matrix = np.array([embeddings[node] for node in G.nodes()])
node_labels = list(G.nodes())

# Assuming you have the nodes from your graph
node_labels = list(G.nodes())  # Get all node labels from the graph

# Ensure the node labels are in the same order as your embedding matrix
assert len(node_labels) == embedding_matrix.shape[0], "Mismatch between number of labels and embeddings."

# If needed, adjust the order of node_labels to match the order of the embeddings

def visualize_embeddings(embedding_matrix, node_labels):
    # Reduce dimensions to 2D using t-SNE
    tsne = TSNE(n_components=2, random_state=42)
    reduced_embeddings = tsne.fit_transform(embedding_matrix)

    # Define colors based on the prefix of unit names
    program_colors = {'MDS': 'blue', 'MST': 'red', 'MDA': 'green'}
    colors = []

    for node in node_labels:
        if node.startswith('MDS'):
            colors.append(program_colors['MDS'])
        elif node.startswith('MST'):
            colors.append(program_colors['MST'])
        elif node.startswith('MDA'):
            colors.append(program_colors['MDA'])
        else:
            colors.append('gray')  # Default color for nodes that don't match any program

    # Ensure the number of colors matches the number of nodes
    assert len(colors) == len(reduced_embeddings), "Mismatch between number of colors and nodes."

    # Plot the 2D embeddings with different colors for each program
    plt.figure(figsize=(12, 12))
    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], alpha=0.7, c=colors, s=100)

    # Annotate each node with its label
    for i, label in enumerate(node_labels):
        plt.annotate(label, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]), fontsize=5, alpha=0.75)

    plt.title("Node Embeddings Visualized with t-SNE")
    plt.show()

# Visualize embeddings
visualize_embeddings(embedding_matrix, node_labels)

from sklearn.cluster import KMeans

def cluster_nodes(embedding_matrix, num_clusters=3):
    kmeans = KMeans(n_clusters=num_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(embedding_matrix)

    return cluster_labels

# Cluster the nodes into 3 clusters
cluster_labels = cluster_nodes(embedding_matrix, num_clusters=3)

# Visualize the clusters with t-SNE
def visualize_clusters(embedding_matrix, node_labels, cluster_labels):
    tsne = TSNE(n_components=2, random_state=42)
    reduced_embeddings = tsne.fit_transform(embedding_matrix)

    plt.figure(figsize=(12, 12))
    scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], alpha=0.7, c=cluster_labels, cmap='viridis', s=100)
    plt.legend(*scatter.legend_elements(), title="Clusters")

    for i, label in enumerate(node_labels):
        plt.annotate(label, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]), fontsize=5, alpha=0.75)

    plt.title("Node Embeddings Clusters Visualized with t-SNE")
    plt.show()

# Visualize the clusters
visualize_clusters(embedding_matrix, node_labels, cluster_labels)

"""TOPIC CLUSTERING OF SUBTOPICS"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

def topic_clustering_analysis(subtopics, n_clusters=5):
    vectorizer = TfidfVectorizer()
    X = vectorizer.fit_transform(subtopics)

    # Perform K-means clustering
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    kmeans.fit(X)

    # Reduce dimensions for visualization
    pca = PCA(n_components=2)
    scatter_data = pca.fit_transform(X.toarray())

    # Plot the clusters
    plt.figure(figsize=(10, 7))
    plt.scatter(scatter_data[:, 0], scatter_data[:, 1], c=kmeans.labels_, cmap='viridis')
    plt.title(f'Topic Clustering of Subtopics (n_clusters={n_clusters})')
    plt.xlabel('PCA Component 1')
    plt.ylabel('PCA Component 2')
    plt.show()

    # Print cluster labels with their respective subtopics
    clustered_subtopics = defaultdict(list)
    for i, label in enumerate(kmeans.labels_):
        clustered_subtopics[label].append(subtopics[i])

    for cluster, topics in clustered_subtopics.items():
        print(f"\nCluster {cluster}:")
        print(topics)

# Run the analysis
topic_clustering_analysis(all_subtopics, n_clusters=5)

"""Centrality Analysis in the Graph"""

import networkx as nx
import matplotlib.pyplot as plt

def centrality_analysis(combined_course_structure):
    G = nx.Graph()

    # Iterate through the outermost level
    for dept_code, dept_courses in combined_course_structure.items():
        # Iterate through each course in the department
        for course_code, course_data in dept_courses.items():
            if 'units' not in course_data:
                continue  # Skip if 'units' key is not present

            # Iterate through each unit within the course
            for unit_code, unit_data in course_data['units'].items():
                unit_name = unit_data['unit_name']
                G.add_node(unit_name, label='unit')  # Add the unit as a node
                G.add_edge(course_code, unit_name)  # Connect the course to the unit

                # Add subtopics and connect them to the unit
                for subtopic in unit_data['subtopics']:
                    G.add_node(subtopic, label='subtopic')  # Add the subtopic as a node
                    G.add_edge(unit_name, subtopic)  # Connect the unit to the subtopic

    # Calculate the degree centrality
    centrality = nx.degree_centrality(G)

    # Sort by centrality score
    sorted_centrality = sorted(centrality.items(), key=lambda item: item[1], reverse=True)

    # Print the top central nodes
    print("Top central nodes:")
    for node, score in sorted_centrality[:10]:
        print(f"{node}: {score}")

    # Visualize the graph with a different color map
    plt.figure(figsize=(20,20))
    pos = nx.spring_layout(G, seed=42)  # Fixed layout for consistency

    # Increase the node size
    node_size = [v * 30000 for v in centrality.values()]  # Adjust the multiplier for larger nodes
    nodes = nx.draw_networkx_nodes(G, pos, node_size=node_size,
                                   node_color=list(centrality.values()), cmap=plt.cm.Blues)
    nx.draw_networkx_edges(G, pos)
    nx.draw_networkx_labels(G, pos, font_size=5)  # Increase font size for better readability

    plt.title("Graph Centrality Visualization")
    plt.colorbar(nodes)  # Add a colorbar to show the centrality scale
    plt.show()


# Run the analysis
centrality_analysis(combined_course_structure)

"""Keyword Frequency Analysis"""

from collections import Counter
import seaborn as sns
import matplotlib.pyplot as plt

def keyword_frequency_analysis(all_subtopics, top_n=20):
    all_words = ' '.join(all_subtopics).split()
    frequency = Counter(all_words)
    common_words = frequency.most_common(top_n)

    words, counts = zip(*common_words)

    plt.figure(figsize=(12, 6))
    sns.barplot(x=counts, y=words, palette='viridis')
    plt.title(f"Top {top_n} Most Common Keywords in Subtopics")
    plt.xlabel("Frequency")
    plt.ylabel("Keyword")
    plt.show()

# Run the analysis
keyword_frequency_analysis(all_subtopics, top_n=20)

